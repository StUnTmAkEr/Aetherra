"""
ğŸ§ª Meta-Reasoning Engine Test Suite
Comprehensive tests for the MetaReasoningEngine functionality
"""

import sys
import os
import time
import asyncio
from pathlib import Path

# Add project path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_meta_reasoning_engine():
    """Test the MetaReasoningEngine with mock systems"""

    print("ğŸ§  Testing Meta-Reasoning Engine")
    print("=" * 40)

    # Mock memory system
    class TestMemorySystem:
        def __init__(self):
            self.stored_traces = []

        def store(self, data):
            self.stored_traces.append(data)
            return True

    # Mock plugin manager
    class TestPluginManager:
        def list_plugin_names(self):
            return ["test_plugin_1", "test_plugin_2", "summarizer", "file_manager"]

    # Initialize systems
    memory = TestMemorySystem()
    plugin_manager = TestPluginManager()

    try:
        # Import the MetaReasoningEngine
        from Aetherra.lyrixa.intelligence.meta_reasoning import MetaReasoningEngine, DecisionType

        # Create engine
        meta_engine = MetaReasoningEngine(memory, plugin_manager)

        print("âœ… Meta-Reasoning Engine initialized successfully")

        # Test 1: Basic decision tracing
        print("\n1ï¸âƒ£ Testing basic decision tracing...")
        trace1 = meta_engine.trace_decision(
            context={"goal": "test", "user_input": "hello"},
            decision="test_plugin_1",
            options=["test_plugin_1", "test_plugin_2"],
            confidence=0.8,
            explanation="Plugin 1 has better test capabilities",
            decision_type=DecisionType.PLUGIN_SELECTION
        )
        print(f"   âœ… Created trace: {trace1.trace_id}")
        print(f"   ğŸ“Š Confidence: {trace1.confidence}")
        print(f"   ğŸ·ï¸ Confidence Level: {trace1.confidence_level.label}")

        # Test 2: Plugin choice explanation
        print("\n2ï¸âƒ£ Testing plugin choice explanation...")
        trace2 = meta_engine.explain_plugin_choice(
            goal="summarize user data",
            context_summary="User wants overview of recent activities",
            plugin_chosen="summarizer",
            reason="Summarizer plugin is optimized for data aggregation",
            confidence=0.9,
            memory_links=["recent_summaries", "user_preferences"],
            intent="data_summarization"
        )
        print(f"   âœ… Plugin choice traced: {trace2.trace_id}")
        print(f"   ğŸ¯ Intent: {trace2.context['intent']}")
        print(f"   ğŸ”— Memory links: {trace2.context['memory_links']}")

        # Test 3: Goal planning explanation
        print("\n3ï¸âƒ£ Testing goal planning explanation...")
        planned_steps = ["analyze_request", "gather_data", "format_response"]
        trace3 = meta_engine.explain_goal_planning(
            user_request="Show me my progress this week",
            planned_steps=planned_steps,
            confidence=0.85,
            reasoning="Multi-step approach needed for comprehensive progress analysis"
        )
        print(f"   âœ… Goal planning traced: {trace3.trace_id}")
        print(f"   ğŸ“‹ Steps count: {trace3.context['steps_count']}")
        print(f"   ğŸ¯ Complexity: {trace3.context['complexity_estimate']}")

        # Test 4: Answer generation explanation
        print("\n4ï¸âƒ£ Testing answer generation explanation...")
        trace4 = meta_engine.explain_answer_generation(
            question="What did I accomplish yesterday?",
            answer_approach="memory_based_summary",
            confidence=0.75,
            sources_used=["daily_logs", "completed_tasks", "time_tracking"],
            reasoning="Using memory-based approach for temporal query"
        )
        print(f"   âœ… Answer generation traced: {trace4.trace_id}")
        print(f"   ğŸ“š Sources: {trace4.metadata['sources_used']}")
        print(f"   â“ Question type: {trace4.context['question_type']}")

        # Test 5: Feedback and learning
        print("\n5ï¸âƒ£ Testing feedback and learning...")
        feedback_success = meta_engine.add_feedback(
            trace_id=trace2.trace_id,
            feedback_score=0.9,
            feedback_text="Excellent summarization, very helpful"
        )
        print(f"   âœ… Feedback added: {feedback_success}")

        # Test 6: Reflection
        print("\n6ï¸âƒ£ Testing reflection...")
        reflection_success = meta_engine.reflect_on_decision(
            trace_id=trace3.trace_id,
            outcome="Successfully provided comprehensive progress summary",
            learned="User prefers detailed breakdown of accomplishments"
        )
        print(f"   âœ… Reflection completed: {reflection_success}")

        # Test 7: History and analytics
        print("\n7ï¸âƒ£ Testing history and analytics...")
        history = meta_engine.get_reasoning_history(5)
        print(f"   ğŸ“š History entries: {len(history)}")

        trends = meta_engine.get_confidence_trends()
        print(f"   ğŸ“ˆ Confidence trends: {trends}")

        report = meta_engine.generate_reasoning_report()
        print(f"   ğŸ“Š Total decisions: {report['total_decisions']}")
        print(f"   ğŸ“Š Average confidence: {report['average_confidence']:.2f}")

        # Test 8: Memory storage verification
        print("\n8ï¸âƒ£ Testing memory storage...")
        print(f"   ğŸ’¾ Traces stored in memory: {len(memory.stored_traces)}")

        # Show sample trace structure
        if memory.stored_traces:
            sample_trace = memory.stored_traces[0]
            print(f"   ğŸ“‹ Sample trace type: {sample_trace.get('type')}")
            print(f"   ğŸ“‹ Sample decision: {sample_trace.get('decision')}")

        print("\nğŸ‰ All tests passed successfully!")
        print(f"ğŸ“Š Summary:")
        print(f"   â€¢ Decisions tracked: {len(meta_engine.decision_history)}")
        print(f"   â€¢ Memory entries: {len(memory.stored_traces)}")
        print(f"   â€¢ Learning patterns: {len(meta_engine.learning_patterns)}")

        return True

    except ImportError as e:
        print(f"[ERROR] Import failed: {e}")
        print("ğŸ’¡ Make sure meta_reasoning.py is in the correct location")
        return False
    except Exception as e:
        print(f"[ERROR] Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_decision_types_and_confidence():
    """Test decision types and confidence level classification"""

    print("\nğŸ” Testing Decision Types and Confidence Levels")
    print("=" * 45)

    try:
        from Aetherra.lyrixa.intelligence.meta_reasoning import DecisionType, ConfidenceLevel

        # Test decision types
        print("ğŸ“ Available Decision Types:")
        for decision_type in DecisionType:
            print(f"   â€¢ {decision_type.value}")

        # Test confidence levels
        print("\nğŸ¯ Confidence Level Classification:")
        test_scores = [0.1, 0.3, 0.5, 0.7, 0.9]
        for score in test_scores:
            level = ConfidenceLevel.from_score(score)
            print(f"   Score {score} â†’ {level.label} ({level.min_val}-{level.max_val})")

        print("   âœ… Decision types and confidence levels working correctly")

        return True

    except Exception as e:
        print(f"[ERROR] Decision types test failed: {e}")
        return False


async def test_integration_example():
    """Test the integration example"""

    print("\nğŸ”— Testing Integration Example")
    print("=" * 35)

    try:
        # Import and run a simple integration test
        from meta_reasoning_integration_example import MetaReasoningDemo

        print("ğŸš€ Running integration demo...")
        demo = MetaReasoningDemo()

        # Run a single test case
        conversation_manager = demo.conversation_manager
        result = await conversation_manager.process_user_request("Can you help me summarize my data?")

        print(f"   âœ… Integration test completed")
        print(f"   ğŸ“ Response: {result[:50]}...")

        # Check reasoning history
        meta_engine = conversation_manager.meta_reasoning
        history = meta_engine.get_reasoning_history(3)
        print(f"   ğŸ“š Reasoning traces created: {len(history)}")

        return True

    except Exception as e:
        print(f"[ERROR] Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("ğŸ§ª Meta-Reasoning Engine Test Suite")
    print("=" * 50)

    # Run all tests
    test1_passed = test_meta_reasoning_engine()
    test2_passed = test_decision_types_and_confidence()

    # Run async integration test
    try:
        test3_passed = asyncio.run(test_integration_example())
    except Exception as e:
        print(f"[ERROR] Async test failed: {e}")
        test3_passed = False

    # Final results
    print("\n" + "=" * 50)
    print("ğŸ Test Results Summary:")
    print(f"   â€¢ Core Engine Test: {'âœ… PASSED' if test1_passed else '[ERROR] FAILED'}")
    print(f"   â€¢ Types & Confidence: {'âœ… PASSED' if test2_passed else '[ERROR] FAILED'}")
    print(f"   â€¢ Integration Test: {'âœ… PASSED' if test3_passed else '[ERROR] FAILED'}")

    if all([test1_passed, test2_passed, test3_passed]):
        print("\nğŸ‰ All tests passed! Meta-Reasoning Engine is ready for production.")
    else:
        print("\n[WARN] Some tests failed. Check the errors above.")
