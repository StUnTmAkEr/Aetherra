#!/usr/bin/env python3
"""
‚ùì Self-Question Generator - Autonomous Question Creation Engine

This module generates intelligent, specific questions based on insights from
story_model.py and reflect_analyzer.py. It creates questions that drive
self-directed learning and knowledge exploration.

Features:
- Story-based question generation from narrative patterns
- Reflection-driven questioning from pattern analysis
- Adaptive question complexity based on knowledge gaps
- Question prioritization and scheduling
"""

import asyncio
import json
import logging
import re
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List

# Try to import memory components with fallbacks
try:
    from ..memory.fractal_mesh.timelines.reflective_timeline_engine import (
        ReflectiveTimelineEngine,
    )
    from ..memory.narrator.story_model import MemoryNarrator
    from ..memory.reflector.reflect_analyzer import ReflectAnalyzer
    from .curiosity_agent import CuriosityAgent, CuriosityQuestion, KnowledgeGap
except ImportError:
    print("‚ö†Ô∏è Memory components not available, using mock implementations")
    MemoryNarrator = None
    ReflectAnalyzer = None
    ReflectiveTimelineEngine = None
    CuriosityAgent = None
    KnowledgeGap = None
    CuriosityQuestion = None


@dataclass
class QuestionInsight:
    """Represents insight that led to question generation"""

    insight_id: str
    source: str  # "story", "reflection", "pattern", "gap", "contradiction"
    insight_text: str
    confidence: float
    timestamp: str
    related_memories: List[str]


@dataclass
class GeneratedQuestion:
    """A question generated by the self-question system"""

    question_id: str
    question_text: str
    question_category: str  # "understanding", "prediction", "optimization", "exploration", "validation"
    complexity_level: str  # "basic", "intermediate", "advanced", "meta"
    insight_source: QuestionInsight
    priority_score: float
    estimated_exploration_time: str
    success_criteria: List[str]
    follow_up_questions: List[str]
    timestamp: str
    status: str = (
        "generated"  # "generated", "scheduled", "exploring", "answered", "abandoned"
    )


class SelfQuestionGenerator:
    """
    Generates autonomous questions based on narrative and reflection insights
    """

    def __init__(self, memory_engine=None, data_dir="question_data"):
        self.memory_engine = memory_engine
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # Initialize components
        self.narrator = None
        self.reflector = None
        self.timeline_engine = None
        self.curiosity_agent = None

        if memory_engine:
            self._initialize_components()

        # Storage
        self.generated_questions: Dict[str, GeneratedQuestion] = {}
        self.question_insights: Dict[str, QuestionInsight] = {}

        # Configuration
        self.max_questions_per_session = 10
        self.complexity_progression = True
        self.question_diversity_threshold = 0.3

        # Question templates by category
        self.question_templates = self._initialize_question_templates()

        # Load existing data
        self._load_persistence_data()

        logging.info("‚ùì Self-Question Generator initialized")

    def _initialize_components(self):
        """Initialize memory system components"""
        try:
            if MemoryNarrator:
                self.narrator = MemoryNarrator()
            if ReflectAnalyzer:
                self.reflector = ReflectAnalyzer()
            if ReflectiveTimelineEngine:
                self.timeline_engine = ReflectiveTimelineEngine()
            if CuriosityAgent:
                self.curiosity_agent = CuriosityAgent(self.memory_engine)

            logging.info("‚úÖ Question generator components initialized")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Could not initialize components: {e}")

    def _initialize_question_templates(self) -> Dict[str, List[str]]:
        """Initialize question templates for different categories"""
        return {
            "understanding": [
                "What is the fundamental nature of {concept}?",
                "How does {concept} relate to {related_concept}?",
                "What are the key characteristics that define {concept}?",
                "Under what conditions does {concept} apply?",
                "What exceptions or edge cases exist for {concept}?",
            ],
            "prediction": [
                "What factors determine when {outcome} will occur?",
                "How can I predict {outcome} more accurately?",
                "What early indicators suggest {outcome} is likely?",
                "What variables most strongly influence {outcome}?",
                "Under what conditions does {pattern} break down?",
            ],
            "optimization": [
                "How can I improve {process} performance?",
                "What bottlenecks limit {process} effectiveness?",
                "What trade-offs exist in optimizing {process}?",
                "What alternative approaches might work better for {process}?",
                "How can I measure success in improving {process}?",
            ],
            "exploration": [
                "What aspects of {domain} remain unexplored?",
                "What would happen if I changed {variable} in {context}?",
                "What connections exist between {domain1} and {domain2}?",
                "What new possibilities emerge if {assumption} is false?",
                "What patterns might I discover by examining {data_source}?",
            ],
            "validation": [
                "How can I verify that {belief} is accurate?",
                "What evidence would contradict {assumption}?",
                "How robust is {conclusion} to different conditions?",
                "What alternative explanations exist for {observation}?",
                "How can I test the limits of {principle}?",
            ],
            "meta": [
                "How am I learning about {domain}?",
                "What biases might affect my understanding of {topic}?",
                "How has my thinking about {concept} evolved?",
                "What learning strategies work best for {type_of_knowledge}?",
                "How can I improve my reasoning about {complex_topic}?",
            ],
        }

    async def generate_questions_from_stories(
        self, timeframe_hours: int = 24
    ) -> List[GeneratedQuestion]:
        """
        Generate questions based on recent narrative insights

        Args:
            timeframe_hours: How far back to analyze stories

        Returns:
            List of generated questions
        """
        questions = []

        if not self.narrator:
            logging.warning("‚ö†Ô∏è Story narrator not available")
            return questions

        try:
            # Get recent stories (mock implementation)
            recent_stories = await self._get_recent_stories(timeframe_hours)

            for story in recent_stories:
                # Extract insights from story
                story_insights = await self._extract_story_insights(story)

                for insight in story_insights:
                    # Generate questions from insight
                    insight_questions = await self._generate_questions_from_insight(
                        insight
                    )
                    questions.extend(insight_questions)

            logging.info(f"‚ùì Generated {len(questions)} questions from stories")

        except Exception as e:
            logging.error(f"‚ùå Error generating questions from stories: {e}")

        return questions

    async def generate_questions_from_reflections(
        self, timeframe_hours: int = 24
    ) -> List[GeneratedQuestion]:
        """
        Generate questions based on recent reflection patterns

        Args:
            timeframe_hours: How far back to analyze reflections

        Returns:
            List of generated questions
        """
        questions = []

        if not self.reflector:
            logging.warning("‚ö†Ô∏è Reflector not available")
            return questions

        try:
            # Get recent reflection patterns (mock implementation)
            reflection_patterns = await self._get_recent_reflection_patterns(
                timeframe_hours
            )

            for pattern in reflection_patterns:
                # Extract insights from pattern
                pattern_insights = await self._extract_pattern_insights(pattern)

                for insight in pattern_insights:
                    # Generate questions from insight
                    insight_questions = await self._generate_questions_from_insight(
                        insight
                    )
                    questions.extend(insight_questions)

            logging.info(f"‚ùì Generated {len(questions)} questions from reflections")

        except Exception as e:
            logging.error(f"‚ùå Error generating questions from reflections: {e}")

        return questions

    async def generate_questions_from_gaps(
        self, knowledge_gaps: List[KnowledgeGap] = None
    ) -> List[GeneratedQuestion]:
        """
        Generate questions based on identified knowledge gaps

        Args:
            knowledge_gaps: Specific gaps to generate questions for

        Returns:
            List of generated questions
        """
        questions = []

        try:
            # Get knowledge gaps if not provided
            if knowledge_gaps is None and self.curiosity_agent:
                knowledge_gaps = await self.curiosity_agent.detect_knowledge_gaps()
            elif knowledge_gaps is None:
                knowledge_gaps = await self._mock_knowledge_gaps()

            for gap in knowledge_gaps:
                # Create insight from gap
                gap_insight = QuestionInsight(
                    insight_id=f"gap_insight_{gap.gap_id}",
                    source="gap",
                    insight_text=f"Knowledge gap identified: {gap.description}",
                    confidence=gap.confidence_score,
                    timestamp=datetime.now().isoformat(),
                    related_memories=gap.related_memories,
                )

                # Store insight
                self.question_insights[gap_insight.insight_id] = gap_insight

                # Generate questions for gap
                gap_questions = await self._generate_gap_specific_questions(
                    gap, gap_insight
                )
                questions.extend(gap_questions)

            logging.info(f"‚ùì Generated {len(questions)} questions from knowledge gaps")

        except Exception as e:
            logging.error(f"‚ùå Error generating questions from gaps: {e}")

        return questions

    async def _get_recent_stories(self, timeframe_hours: int) -> List[Dict[str, Any]]:
        """Get recent stories from narrator"""
        # Mock implementation - in production, this would query the narrator
        return [
            {
                "story_id": "story_1",
                "narrative": "Over the past week, I experienced a breakthrough in plugin management. Initially struggling with async plugin loading and race conditions, I developed systematic debugging strategies.",
                "insights": [
                    "systematic debugging",
                    "async challenges",
                    "performance improvement",
                ],
                "emotional_arc": "struggle -> insight -> achievement",
                "confidence": 0.8,
                "timestamp": datetime.now().isoformat(),
            },
            {
                "story_id": "story_2",
                "narrative": "Memory system optimization led to 3x performance improvement through better indexing and caching strategies.",
                "insights": [
                    "optimization success",
                    "indexing benefits",
                    "caching strategies",
                ],
                "emotional_arc": "problem -> solution -> validation",
                "confidence": 0.9,
                "timestamp": (datetime.now() - timedelta(hours=2)).isoformat(),
            },
        ]

    async def _get_recent_reflection_patterns(
        self, timeframe_hours: int
    ) -> List[Dict[str, Any]]:
        """Get recent reflection patterns from reflector"""
        # Mock implementation - in production, this would query the reflector
        return [
            {
                "pattern_id": "pattern_1",
                "pattern_type": "learning_acceleration",
                "description": "Learning accelerates when systematic debugging is combined with iterative testing",
                "confidence": 0.85,
                "occurrences": 5,
                "related_memories": ["debug_mem_1", "test_mem_2", "learn_mem_3"],
                "timestamp": datetime.now().isoformat(),
            },
            {
                "pattern_id": "pattern_2",
                "pattern_type": "performance_correlation",
                "description": "Memory system performance correlates strongly with indexing strategy choice",
                "confidence": 0.9,
                "occurrences": 8,
                "related_memories": ["perf_mem_1", "index_mem_2", "strat_mem_3"],
                "timestamp": (datetime.now() - timedelta(hours=1)).isoformat(),
            },
        ]

    async def _mock_knowledge_gaps(self) -> List[KnowledgeGap]:
        """Mock knowledge gaps for testing"""
        if not KnowledgeGap:
            return []

        return [
            KnowledgeGap(
                gap_id="gap_1",
                category="conceptual",
                description="Understanding when async provides performance benefits",
                confidence_score=0.7,
                priority="high",
                related_memories=["async_mem_1", "perf_mem_2"],
                questions=["Why does async help in some cases but not others?"],
                exploration_strategies=["controlled_testing", "pattern_analysis"],
                timestamp=datetime.now().isoformat(),
            )
        ]

    async def _extract_story_insights(
        self, story: Dict[str, Any]
    ) -> List[QuestionInsight]:
        """Extract insights from a story narrative"""
        insights = []

        try:
            # Extract key concepts and insights from story
            narrative = story["narrative"]
            story_insights = story.get("insights", [])

            # Create insights for key narrative elements
            for insight_text in story_insights:
                insight = QuestionInsight(
                    insight_id=f"story_insight_{story['story_id']}_{len(insights)}",
                    source="story",
                    insight_text=f"Story insight: {insight_text}",
                    confidence=story.get("confidence", 0.5),
                    timestamp=story["timestamp"],
                    related_memories=[story["story_id"]],
                )
                insights.append(insight)
                self.question_insights[insight.insight_id] = insight

            # Extract emotional arc insights
            emotional_arc = story.get("emotional_arc", "")
            if emotional_arc:
                arc_insight = QuestionInsight(
                    insight_id=f"arc_insight_{story['story_id']}",
                    source="story",
                    insight_text=f"Emotional progression: {emotional_arc}",
                    confidence=story.get("confidence", 0.5),
                    timestamp=story["timestamp"],
                    related_memories=[story["story_id"]],
                )
                insights.append(arc_insight)
                self.question_insights[arc_insight.insight_id] = arc_insight

        except Exception as e:
            logging.error(f"‚ùå Error extracting story insights: {e}")

        return insights

    async def _extract_pattern_insights(
        self, pattern: Dict[str, Any]
    ) -> List[QuestionInsight]:
        """Extract insights from reflection patterns"""
        insights = []

        try:
            # Create insight from pattern
            insight = QuestionInsight(
                insight_id=f"pattern_insight_{pattern['pattern_id']}",
                source="reflection",
                insight_text=f"Pattern identified: {pattern['description']}",
                confidence=pattern.get("confidence", 0.5),
                timestamp=pattern["timestamp"],
                related_memories=pattern.get("related_memories", []),
            )
            insights.append(insight)
            self.question_insights[insight.insight_id] = insight

            # Create insight for pattern strength if applicable
            occurrences = pattern.get("occurrences", 0)
            if occurrences > 3:
                strength_insight = QuestionInsight(
                    insight_id=f"strength_insight_{pattern['pattern_id']}",
                    source="reflection",
                    insight_text=f"Strong pattern detected ({occurrences} occurrences): {pattern['pattern_type']}",
                    confidence=min(0.95, pattern.get("confidence", 0.5) + 0.1),
                    timestamp=pattern["timestamp"],
                    related_memories=pattern.get("related_memories", []),
                )
                insights.append(strength_insight)
                self.question_insights[strength_insight.insight_id] = strength_insight

        except Exception as e:
            logging.error(f"‚ùå Error extracting pattern insights: {e}")

        return insights

    async def _generate_questions_from_insight(
        self, insight: QuestionInsight
    ) -> List[GeneratedQuestion]:
        """Generate specific questions from an insight"""
        questions = []

        try:
            # Determine question categories based on insight source
            categories = self._determine_question_categories(insight)

            for category in categories:
                # Generate questions for this category
                category_questions = await self._generate_category_questions(
                    insight, category
                )
                questions.extend(category_questions)

            # Limit questions per insight
            questions = questions[:3]  # Max 3 questions per insight

        except Exception as e:
            logging.error(f"‚ùå Error generating questions from insight: {e}")

        return questions

    async def _generate_gap_specific_questions(
        self, gap: KnowledgeGap, insight: QuestionInsight
    ) -> List[GeneratedQuestion]:
        """Generate questions specifically for knowledge gaps"""
        questions = []

        try:
            # Use gap category to determine question types
            category_mapping = {
                "conceptual": "understanding",
                "causal": "prediction",
                "contextual": "exploration",
                "experiential": "optimization",
            }

            primary_category = category_mapping.get(gap.category, "understanding")

            # Generate primary questions
            primary_questions = await self._generate_category_questions(
                insight, primary_category
            )
            questions.extend(primary_questions[:2])

            # Generate validation questions for high-priority gaps
            if gap.priority == "high":
                validation_questions = await self._generate_category_questions(
                    insight, "validation"
                )
                questions.extend(validation_questions[:1])

        except Exception as e:
            logging.error(f"‚ùå Error generating gap-specific questions: {e}")

        return questions

    def _determine_question_categories(self, insight: QuestionInsight) -> List[str]:
        """Determine which question categories to use for an insight"""
        categories = []

        insight_text = insight.insight_text.lower()

        # Understanding questions for conceptual insights
        if any(
            word in insight_text
            for word in ["concept", "definition", "nature", "understanding"]
        ):
            categories.append("understanding")

        # Prediction questions for pattern insights
        if any(
            word in insight_text for word in ["pattern", "correlation", "when", "if"]
        ):
            categories.append("prediction")

        # Optimization questions for performance insights
        if any(
            word in insight_text
            for word in ["performance", "improvement", "optimization", "better"]
        ):
            categories.append("optimization")

        # Exploration questions for gap insights
        if any(
            word in insight_text for word in ["gap", "unknown", "unexplored", "missing"]
        ):
            categories.append("exploration")

        # Validation questions for confident insights
        if insight.confidence > 0.8:
            categories.append("validation")

        # Meta questions for learning insights
        if any(
            word in insight_text
            for word in ["learning", "thinking", "reasoning", "strategy"]
        ):
            categories.append("meta")

        # Default to understanding if no specific category
        if not categories:
            categories.append("understanding")

        return categories

    async def _generate_category_questions(
        self, insight: QuestionInsight, category: str
    ) -> List[GeneratedQuestion]:
        """Generate questions for a specific category"""
        questions = []

        try:
            templates = self.question_templates.get(category, [])

            # Extract key concepts from insight
            concepts = self._extract_concepts_from_insight(insight)

            for template in templates[:2]:  # Max 2 templates per category
                # Fill template with concepts
                filled_question = self._fill_question_template(template, concepts)

                if filled_question:
                    # Create question object
                    question = GeneratedQuestion(
                        question_id=f"q_{insight.insight_id}_{category}_{len(questions)}",
                        question_text=filled_question,
                        question_category=category,
                        complexity_level=self._determine_complexity_level(
                            filled_question, insight
                        ),
                        insight_source=insight,
                        priority_score=self._calculate_priority_score(
                            insight, category
                        ),
                        estimated_exploration_time=self._estimate_exploration_time(
                            category
                        ),
                        success_criteria=self._generate_success_criteria(category),
                        follow_up_questions=self._generate_follow_up_questions(
                            filled_question, category
                        ),
                        timestamp=datetime.now().isoformat(),
                    )

                    questions.append(question)
                    self.generated_questions[question.question_id] = question

        except Exception as e:
            logging.error(f"‚ùå Error generating category questions: {e}")

        return questions

    def _extract_concepts_from_insight(
        self, insight: QuestionInsight
    ) -> Dict[str, str]:
        """Extract key concepts from insight text"""
        text = insight.insight_text.lower()

        concepts = {}

        # Extract technical concepts
        tech_patterns = [
            r"(async|synchronous|concurrent)",
            r"(performance|optimization|efficiency)",
            r"(plugin|module|component)",
            r"(memory|storage|cache|index)",
            r"(pattern|strategy|approach)",
            r"(debugging|testing|validation)",
        ]

        for pattern in tech_patterns:
            matches = re.findall(pattern, text)
            if matches:
                concept_key = pattern.strip("()").split("|")[0]
                concepts[concept_key] = matches[0]

        # Set default concepts if none found
        if not concepts:
            concepts = {
                "concept": "this concept",
                "process": "this process",
                "domain": "this area",
                "outcome": "this outcome",
            }

        return concepts

    def _fill_question_template(self, template: str, concepts: Dict[str, str]) -> str:
        """Fill question template with extracted concepts"""
        try:
            # Find placeholders in template
            placeholders = re.findall(r"{(\w+)}", template)

            filled_template = template

            for placeholder in placeholders:
                # Find best matching concept
                if placeholder in concepts:
                    value = concepts[placeholder]
                elif "concept" in concepts:
                    value = concepts["concept"]
                else:
                    value = f"this {placeholder}"

                filled_template = filled_template.replace(f"{{{placeholder}}}", value)

            return filled_template

        except Exception as e:
            logging.error(f"‚ùå Error filling template: {e}")
            return ""

    def _determine_complexity_level(
        self, question: str, insight: QuestionInsight
    ) -> str:
        """Determine the complexity level of a question"""
        question_lower = question.lower()

        # Meta questions are typically advanced
        if any(
            word in question_lower for word in ["how am i", "what biases", "how has my"]
        ):
            return "meta"

        # Complex questions involve multiple concepts or relationships
        if any(
            word in question_lower
            for word in ["relationship", "correlation", "interaction", "trade-off"]
        ):
            return "advanced"

        # Intermediate questions involve analysis or prediction
        if any(
            word in question_lower
            for word in ["analyze", "predict", "determine", "optimize"]
        ):
            return "intermediate"

        # Basic questions ask for definitions or simple facts
        return "basic"

    def _calculate_priority_score(
        self, insight: QuestionInsight, category: str
    ) -> float:
        """Calculate priority score for a question"""
        base_score = insight.confidence

        # Category priority weights
        category_weights = {
            "understanding": 0.8,
            "prediction": 1.0,
            "optimization": 1.2,
            "exploration": 0.9,
            "validation": 1.1,
            "meta": 0.7,
        }

        # Source priority weights
        source_weights = {
            "story": 1.0,
            "reflection": 1.1,
            "gap": 1.3,
            "pattern": 1.2,
            "contradiction": 1.4,
        }

        return (
            base_score
            * category_weights.get(category, 1.0)
            * source_weights.get(insight.source, 1.0)
        )

    def _estimate_exploration_time(self, category: str) -> str:
        """Estimate exploration time for question category"""
        time_estimates = {
            "understanding": "15-30 minutes",
            "prediction": "30-60 minutes",
            "optimization": "1-2 hours",
            "exploration": "1-3 hours",
            "validation": "30-45 minutes",
            "meta": "45-60 minutes",
        }
        return time_estimates.get(category, "30-60 minutes")

    def _generate_success_criteria(self, category: str) -> List[str]:
        """Generate success criteria for question category"""
        criteria = {
            "understanding": [
                "Clear definition or explanation obtained",
                "Key characteristics identified",
                "Relationship to other concepts clarified",
            ],
            "prediction": [
                "Predictive model or rules developed",
                "Key variables identified",
                "Accuracy of predictions improved",
            ],
            "optimization": [
                "Performance improvement achieved",
                "Bottlenecks identified and addressed",
                "Measurable results obtained",
            ],
            "exploration": [
                "New insights or patterns discovered",
                "Unexplored areas mapped",
                "Novel connections identified",
            ],
            "validation": [
                "Hypothesis tested with evidence",
                "Confidence level quantified",
                "Alternative explanations considered",
            ],
            "meta": [
                "Learning process analyzed",
                "Metacognitive insights gained",
                "Thinking strategies improved",
            ],
        }
        return criteria.get(category, ["Question answered satisfactorily"])

    def _generate_follow_up_questions(
        self, primary_question: str, category: str
    ) -> List[str]:
        """Generate follow-up questions based on primary question"""
        follow_ups = []

        if category == "understanding":
            follow_ups = [
                "How does this understanding change my approach?",
                "What implications does this have for related areas?",
            ]
        elif category == "prediction":
            follow_ups = [
                "How accurate are these predictions in practice?",
                "What factors could invalidate these predictions?",
            ]
        elif category == "optimization":
            follow_ups = [
                "What are the trade-offs of this optimization?",
                "How can I sustain these improvements?",
            ]
        elif category == "exploration":
            follow_ups = [
                "What new questions emerge from this exploration?",
                "How do these discoveries connect to existing knowledge?",
            ]
        elif category == "validation":
            follow_ups = [
                "What additional evidence would strengthen this conclusion?",
                "Under what conditions might this validation fail?",
            ]
        elif category == "meta":
            follow_ups = [
                "How can I apply this metacognitive insight?",
                "What other areas of my thinking need similar analysis?",
            ]

        return follow_ups

    async def prioritize_and_schedule_questions(
        self, questions: List[GeneratedQuestion]
    ) -> List[GeneratedQuestion]:
        """Prioritize and schedule questions for exploration"""
        if not questions:
            return []

        # Sort by priority score
        sorted_questions = sorted(
            questions, key=lambda q: q.priority_score, reverse=True
        )

        # Apply diversity filter
        diverse_questions = self._apply_diversity_filter(sorted_questions)

        # Limit to session maximum
        scheduled_questions = diverse_questions[: self.max_questions_per_session]

        # Update status
        for question in scheduled_questions:
            question.status = "scheduled"
            self.generated_questions[question.question_id] = question

        # Save to persistence
        self._save_persistence_data()

        logging.info(
            f"üìÖ Prioritized and scheduled {len(scheduled_questions)} questions"
        )
        return scheduled_questions

    def _apply_diversity_filter(
        self, questions: List[GeneratedQuestion]
    ) -> List[GeneratedQuestion]:
        """Apply diversity filter to avoid similar questions"""
        diverse_questions = []

        for question in questions:
            # Check if question is diverse enough
            is_diverse = True

            for existing in diverse_questions:
                similarity = self._calculate_question_similarity(question, existing)
                if similarity > (1.0 - self.question_diversity_threshold):
                    is_diverse = False
                    break

            if is_diverse:
                diverse_questions.append(question)

        return diverse_questions

    def _calculate_question_similarity(
        self, q1: GeneratedQuestion, q2: GeneratedQuestion
    ) -> float:
        """Calculate similarity between two questions"""
        # Simple word overlap similarity
        words1 = set(q1.question_text.lower().split())
        words2 = set(q2.question_text.lower().split())

        intersection = words1 & words2
        union = words1 | words2

        if not union:
            return 0.0

        return len(intersection) / len(union)

    def get_question_generation_summary(self) -> Dict[str, Any]:
        """Get summary of question generation activity"""
        total_questions = len(self.generated_questions)

        # Count by status
        status_counts = {}
        for question in self.generated_questions.values():
            status_counts[question.status] = status_counts.get(question.status, 0) + 1

        # Count by category
        category_counts = {}
        for question in self.generated_questions.values():
            category_counts[question.question_category] = (
                category_counts.get(question.question_category, 0) + 1
            )

        # Count by complexity
        complexity_counts = {}
        for question in self.generated_questions.values():
            complexity_counts[question.complexity_level] = (
                complexity_counts.get(question.complexity_level, 0) + 1
            )

        return {
            "timestamp": datetime.now().isoformat(),
            "total_questions": total_questions,
            "total_insights": len(self.question_insights),
            "status_breakdown": status_counts,
            "category_breakdown": category_counts,
            "complexity_breakdown": complexity_counts,
            "average_priority": sum(
                q.priority_score for q in self.generated_questions.values()
            )
            / max(total_questions, 1),
            "recent_questions": [
                q.question_text
                for q in sorted(
                    self.generated_questions.values(),
                    key=lambda x: x.timestamp,
                    reverse=True,
                )[:3]
            ],
        }

    def _load_persistence_data(self):
        """Load persistent data from files"""
        try:
            # Load generated questions
            questions_file = self.data_dir / "generated_questions.json"
            if questions_file.exists():
                with open(questions_file, "r") as f:
                    questions_data = json.load(f)
                    self.generated_questions = {
                        q_id: GeneratedQuestion(**q_data)
                        for q_id, q_data in questions_data.items()
                    }

            # Load question insights
            insights_file = self.data_dir / "question_insights.json"
            if insights_file.exists():
                with open(insights_file, "r") as f:
                    insights_data = json.load(f)
                    self.question_insights = {
                        i_id: QuestionInsight(**i_data)
                        for i_id, i_data in insights_data.items()
                    }

            logging.info(
                f"üìÇ Loaded {len(self.generated_questions)} questions and {len(self.question_insights)} insights"
            )

        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Could not load persistence data: {e}")

    def _save_persistence_data(self):
        """Save persistent data to files"""
        try:
            # Save generated questions
            questions_file = self.data_dir / "generated_questions.json"
            with open(questions_file, "w") as f:
                questions_data = {
                    q_id: asdict(question)
                    for q_id, question in self.generated_questions.items()
                }
                json.dump(questions_data, f, indent=2)

            # Save question insights
            insights_file = self.data_dir / "question_insights.json"
            with open(insights_file, "w") as f:
                insights_data = {
                    i_id: asdict(insight)
                    for i_id, insight in self.question_insights.items()
                }
                json.dump(insights_data, f, indent=2)

            logging.info("üíæ Saved question generation data to persistence")

        except Exception as e:
            logging.error(f"‚ùå Could not save persistence data: {e}")


# Convenience functions for integration
async def generate_questions_from_memories(
    memory_engine=None, timeframe_hours: int = 24
) -> List[GeneratedQuestion]:
    """Convenience function to generate questions from memory analysis"""
    generator = SelfQuestionGenerator(memory_engine)

    # Generate from multiple sources
    story_questions = await generator.generate_questions_from_stories(timeframe_hours)
    reflection_questions = await generator.generate_questions_from_reflections(
        timeframe_hours
    )
    gap_questions = await generator.generate_questions_from_gaps()

    all_questions = story_questions + reflection_questions + gap_questions

    # Prioritize and schedule
    return await generator.prioritize_and_schedule_questions(all_questions)


if __name__ == "__main__":

    async def demo():
        """Demo the Self-Question Generator"""
        print("‚ùì Self-Question Generator Demo")
        print("=" * 50)

        # Initialize generator
        generator = SelfQuestionGenerator()

        # Generate questions from different sources
        print("\nüìñ Generating questions from stories...")
        story_questions = await generator.generate_questions_from_stories(24)

        print(f"\nüîç Generating questions from reflections...")
        reflection_questions = await generator.generate_questions_from_reflections(24)

        print(f"\nüï≥Ô∏è Generating questions from knowledge gaps...")
        gap_questions = await generator.generate_questions_from_gaps()

        # Combine and prioritize
        all_questions = story_questions + reflection_questions + gap_questions
        scheduled_questions = await generator.prioritize_and_schedule_questions(
            all_questions
        )

        # Display results
        print(f"\nüìä Question Generation Results:")
        print(f"   Story questions: {len(story_questions)}")
        print(f"   Reflection questions: {len(reflection_questions)}")
        print(f"   Gap questions: {len(gap_questions)}")
        print(f"   Total generated: {len(all_questions)}")
        print(f"   Scheduled for exploration: {len(scheduled_questions)}")

        print(f"\nüéØ Top Scheduled Questions:")
        for i, question in enumerate(scheduled_questions[:5], 1):
            print(f"   {i}. [{question.question_category}] {question.question_text}")
            print(
                f"      Priority: {question.priority_score:.2f} | Complexity: {question.complexity_level}"
            )
            print(f"      Estimated time: {question.estimated_exploration_time}")

        # Show summary
        summary = generator.get_question_generation_summary()
        print(f"\nüìà Generation Summary:")
        print(f"   Total questions: {summary['total_questions']}")
        print(f"   Average priority: {summary['average_priority']:.2f}")
        print(f"   Categories: {list(summary['category_breakdown'].keys())}")

        print("\nüéâ Demo completed!")

    # Run demo
    asyncio.run(demo())
