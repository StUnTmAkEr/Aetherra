#!/usr/bin/env python3
"""
ğŸ”„ğŸ“ˆ LYRIXA FEEDBACK SYSTEM INTEGRATION TEST
==========================================

Comprehensive test script for Lyrixa's Feedback + Self-Improvement System.
Tests all feedback collection, learning, and adaptation capabilities.
"""

import asyncio
import os
import sys
from datetime import datetime

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from lyrixa.assistant import LyrixaAI
from lyrixa.core.feedback_system import FeedbackRating, FeedbackType


async def test_feedback_system_comprehensive():
    """Comprehensive test of the feedback + self-improvement system"""

    print("ğŸ”„ğŸ“ˆ LYRIXA FEEDBACK SYSTEM INTEGRATION TEST")
    print("=" * 50)

    # Initialize Lyrixa
    print("\n1. Initializing Lyrixa AI Assistant...")
    lyrixa = LyrixaAI(workspace_path=os.getcwd())
    await lyrixa.initialize()

    print("\nâœ… Lyrixa initialized successfully")
    print(f"   Session ID: {lyrixa.session_id}")
    print(
        f"   Feedback system: {'âœ… Active' if lyrixa.feedback_system else 'âŒ Not available'}"
    )

    # Test 1: Basic feedback collection
    print("\n" + "=" * 50)
    print("2. Testing Basic Feedback Collection")
    print("=" * 50)

    # Test suggestion feedback
    print("\nğŸ“ Testing suggestion feedback...")
    suggestion_feedback = await lyrixa.collect_suggestion_feedback(
        suggestion_id="test_suggestion_1",
        accepted=True,
        rating=4,
        reason="Very helpful for my workflow",
    )
    print(f"   Result: {suggestion_feedback}")

    # Test response feedback
    print("\nğŸ“ Testing response feedback...")
    response_feedback = await lyrixa.collect_response_feedback(
        response_id="test_response_1",
        quality_rating=5,
        helpfulness_rating=4,
        tone_feedback="Great tone, very friendly",
    )
    print(f"   Result: {response_feedback}")

    # Test personality feedback
    print("\nğŸ“ Testing personality feedback...")
    personality_feedback = await lyrixa.collect_personality_feedback(
        persona_rating=4,
        preferred_adjustments={"formality": 0.6, "verbosity": 0.4},
        specific_feedback="Could be slightly more formal",
    )
    print(f"   Result: {personality_feedback}")

    # Test interaction feedback
    print("\nğŸ“ Testing interaction feedback...")
    interaction_feedback = await lyrixa.collect_interaction_feedback(
        proactiveness_rating=3,
        timing_rating=4,
        interruption_feedback="Good timing, but maybe too frequent",
    )
    print(f"   Result: {interaction_feedback}")

    # Test 2: Generic feedback collection
    print("\n" + "=" * 50)
    print("3. Testing Generic Feedback Collection")
    print("=" * 50)

    feedback_types = [
        "suggestion",
        "response",
        "personality",
        "interaction",
        "helpfulness",
    ]

    for i, feedback_type in enumerate(feedback_types):
        print(f"\nğŸ“Š Testing {feedback_type} feedback...")
        result = await lyrixa.collect_user_feedback(
            feedback_type=feedback_type,
            rating=3 + (i % 3),  # Vary ratings 3-5
            context={"test_context": f"test_{feedback_type}_{i}"},
            comment=f"Test feedback for {feedback_type}",
        )
        print(f"   Result: {result}")

    # Test 3: Feedback widgets
    print("\n" + "=" * 50)
    print("4. Testing Feedback Widget Handling")
    print("=" * 50)

    # Test suggestion widget response
    print("\nğŸ›ï¸ Testing suggestion widget response...")
    suggestion_widget_response = {
        "type": "suggestion_feedback",
        "suggestion_id": "widget_suggestion_1",
        "value": "helpful",
        "comment": "This suggestion was very useful",
    }
    widget_result = await lyrixa.handle_feedback_widget_response(
        suggestion_widget_response
    )
    print(f"   Result: {widget_result}")

    # Test response widget response
    print("\nğŸ›ï¸ Testing response widget response...")
    response_widget_response = {
        "type": "response_feedback",
        "response_id": "widget_response_1",
        "quality_rating": 4,
        "helpfulness_rating": 5,
        "tone_feedback": "Perfect tone",
    }
    widget_result = await lyrixa.handle_feedback_widget_response(
        response_widget_response
    )
    print(f"   Result: {widget_result}")

    # Test 4: Performance and adaptive settings
    print("\n" + "=" * 50)
    print("5. Testing Performance Tracking & Adaptive Settings")
    print("=" * 50)

    # Get performance report
    print("\nğŸ“Š Getting performance report...")
    performance_report = await lyrixa.get_performance_report()
    print("   Performance Metrics:")
    if "performance_metrics" in performance_report:
        for metric, value in performance_report["performance_metrics"].items():
            print(f"     {metric}: {value}")

    # Get adaptive settings
    print("\nâš™ï¸ Getting adaptive settings...")
    adaptive_settings = lyrixa.get_current_adaptive_settings()
    print("   Adaptive Parameters:")
    if "adaptive_parameters" in adaptive_settings:
        for param, value in adaptive_settings["adaptive_parameters"].items():
            print(f"     {param}: {value}")

    # Test 5: Proactive feedback requests
    print("\n" + "=" * 50)
    print("6. Testing Proactive Feedback Requests")
    print("=" * 50)

    # Test proactive feedback request
    print("\nğŸ¤– Testing proactive feedback request...")
    feedback_context = {
        "recent_suggestions_count": 5,
        "user_activity": "high",
        "session_duration": 30,
    }
    proactive_request = await lyrixa.request_proactive_feedback(feedback_context)
    print(f"   Proactive request: {proactive_request}")

    # Test with different context
    feedback_context_2 = {
        "recent_suggestions_count": 1,
        "user_activity": "low",
        "session_duration": 5,
    }
    proactive_request_2 = await lyrixa.request_proactive_feedback(feedback_context_2)
    print(f"   Proactive request 2: {proactive_request_2}")

    # Test 6: Brain loop integration
    print("\n" + "=" * 50)
    print("7. Testing Brain Loop Integration")
    print("=" * 50)

    # Test brain loop with feedback integration
    print("\nğŸ§  Testing brain loop with feedback integration...")
    test_inputs = [
        "Generate .aether code for data processing",
        "Help me analyze this dataset",
        "What suggestions do you have for my workflow?",
    ]

    for i, test_input in enumerate(test_inputs):
        print(f"\n   Test {i + 1}: {test_input}")
        brain_result = await lyrixa.brain_loop(test_input, context={"test_mode": True})

        # Check for feedback widgets in GUI updates
        if "gui_updates" in brain_result:
            gui_updates = brain_result["gui_updates"]
            if "feedback_request" in gui_updates:
                print(
                    f"     Feedback request generated: {gui_updates['feedback_request']['type']}"
                )
            if "suggestion_feedback_widgets" in gui_updates:
                print(
                    f"     Suggestion feedback widgets: {len(gui_updates['suggestion_feedback_widgets'])}"
                )
            if "response_feedback_widget" in gui_updates:
                print(f"     Response feedback widget: âœ… Generated")

    # Test 7: Learning and adaptation
    print("\n" + "=" * 50)
    print("8. Testing Learning and Adaptation")
    print("=" * 50)

    # Add more feedback to trigger learning
    print("\nğŸ“š Adding feedback to trigger learning...")
    feedback_scenarios = [
        {"type": "suggestion", "rating": 2, "comment": "Too frequent suggestions"},
        {"type": "suggestion", "rating": 2, "comment": "Not very relevant"},
        {"type": "personality", "rating": 2, "comment": "Too formal"},
        {"type": "interaction", "rating": 1, "comment": "Too interruptive"},
        {"type": "response", "rating": 5, "comment": "Excellent response"},
    ]

    for scenario in feedback_scenarios:
        result = await lyrixa.collect_user_feedback(
            feedback_type=scenario["type"],
            rating=scenario["rating"],
            context={"learning_test": True},
            comment=scenario["comment"],
        )
        print(f"   Added {scenario['type']} feedback: rating {scenario['rating']}")

    # Check adaptive parameters after learning
    print("\nğŸ”„ Checking adaptive parameters after learning...")
    updated_settings = lyrixa.get_current_adaptive_settings()
    print("   Updated Adaptive Parameters:")
    if "adaptive_parameters" in updated_settings:
        for param, value in updated_settings["adaptive_parameters"].items():
            print(f"     {param}: {value}")

    # Test 8: Reset learning
    print("\n" + "=" * 50)
    print("9. Testing Learning Reset")
    print("=" * 50)

    print("\nğŸ”„ Testing learning reset...")
    reset_result = await lyrixa.reset_learning(keep_recent_days=1)
    print(f"   Reset result: {reset_result}")

    # Final performance report
    print("\n" + "=" * 50)
    print("10. Final Performance Report")
    print("=" * 50)

    final_report = await lyrixa.get_performance_report()
    print("\nğŸ“Š Final Performance Report:")

    if "performance_metrics" in final_report:
        print("   Performance Metrics:")
        for metric, value in final_report["performance_metrics"].items():
            print(f"     {metric}: {value}")

    if "feedback_summary" in final_report:
        summary = final_report["feedback_summary"]
        print(f"\n   Feedback Summary:")
        print(f"     Total entries: {summary.get('total_entries', 0)}")
        print(f"     By type: {summary.get('by_type', {})}")

    if "recent_improvements" in final_report:
        improvements = final_report["recent_improvements"]
        print(f"\n   Recent Improvements: {len(improvements)}")
        for improvement in improvements:
            print(
                f"     - {improvement.get('area', 'unknown')}: {improvement.get('action', 'unknown')}"
            )

    print("\n" + "=" * 50)
    print("âœ… FEEDBACK SYSTEM INTEGRATION TEST COMPLETE")
    print("=" * 50)
    print("\nğŸ‰ All feedback and self-improvement features tested successfully!")
    print("   âœ… Basic feedback collection")
    print("   âœ… Widget handling")
    print("   âœ… Performance tracking")
    print("   âœ… Proactive feedback requests")
    print("   âœ… Brain loop integration")
    print("   âœ… Learning and adaptation")
    print("   âœ… Learning reset functionality")

    return True


async def main():
    """Main test execution"""
    try:
        success = await test_feedback_system_comprehensive()
        if success:
            print(f"\nğŸŠ All tests completed successfully at {datetime.now()}")
        else:
            print(f"\nâŒ Some tests failed at {datetime.now()}")
            sys.exit(1)

    except Exception as e:
        print(f"\nğŸ’¥ Test execution failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
