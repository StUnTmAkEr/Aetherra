#!/usr/bin/env python3
"""
ðŸ§  LYRIXA INTELLIGENCE INTEGRATION
=================================

This module integrates all Aetherra AI OS Intelligence Stack components
into Lyrixa, providing real-time awareness and system orchestration.
"""

import asyncio
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import Aetherra runtime components
try:
    from Aetherra.runtime.aether_runtime import AetherRuntime
except ImportError:
    print("âš ï¸ Aetherra runtime not available")
    AetherRuntime = None

# Import conversation manager
try:
    from .conversation_manager import LyrixaConversationManager

    CONVERSATION_MANAGER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Conversation manager not available: {e}")
    LyrixaConversationManager = None
    CONVERSATION_MANAGER_AVAILABLE = False


class LyrixaIntelligenceStack:
    """
    ðŸ§  Lyrixa Intelligence Stack Integration

    This class provides Lyrixa with comprehensive intelligence capabilities
    by integrating all core Aetherra AI OS system plugins and modules.
    """

    def __init__(self, workspace_path: str, aether_runtime=None):
        self.workspace_path = workspace_path
        self.aether_runtime = aether_runtime or (
            AetherRuntime() if AetherRuntime else None
        )

        # Initialize conversation manager
        if CONVERSATION_MANAGER_AVAILABLE and LyrixaConversationManager:
            try:
                self.conversation_manager = LyrixaConversationManager(
                    workspace_path=workspace_path, aether_runtime=self.aether_runtime
                )
                print("âœ… LLM-powered conversation manager initialized")
            except Exception as e:
                print(f"âš ï¸ Failed to initialize conversation manager: {e}")
                self.conversation_manager = None
        else:
            self.conversation_manager = None

        # Intelligence components status
        self.intelligence_status = {
            "semantic_memory": False,
            "system_awareness": False,
            "self_reflection": False,
            "event_correlation": False,
            "conversational_integration": False,
            "plugin_monitoring": False,
        }

        # System workflow status
        self.workflow_status = {
            "goal_autopilot": {"active": False, "last_run": None, "health": "unknown"},
            "agent_sync": {"active": False, "last_run": None, "health": "unknown"},
            "memory_cleanser": {"active": False, "last_run": None, "health": "unknown"},
            "daily_reflector": {"active": False, "last_run": None, "health": "unknown"},
            "plugin_watchdog": {"active": False, "last_run": None, "health": "unknown"},
        }

        # System modules status
        self.module_status = {
            "agents": False,
            "goals": False,
            "plugins": False,
            "logger": False,
            "memory_ops": False,
            "utils": False,
        }

        # Initialize logging
        self.logger = logging.getLogger("lyrixa.intelligence")
        self.logger.setLevel(logging.INFO)

        # Intelligence cache
        self.intelligence_cache = {}
        self.last_intelligence_update = None

        print("ðŸ§  Intelligence Stack initialized")

    async def initialize_intelligence_layer(self) -> Dict[str, Any]:
        """Initialize all intelligence layer components"""
        try:
            print("ðŸ§  Initializing Intelligence Layer...")

            # Check semantic memory with confidence scoring
            await self._check_semantic_memory()

            # Initialize real-time system awareness
            await self._initialize_system_awareness()

            # Setup AI self-reflection capabilities
            await self._setup_self_reflection()

            # Initialize event correlation and reasoning
            await self._initialize_event_correlation()

            # Setup conversational interface integration
            await self._setup_conversational_integration()

            # Initialize plugin monitoring
            await self._initialize_plugin_monitoring()

            self.last_intelligence_update = datetime.now()

            print("âœ… Intelligence Layer initialized successfully")
            return {
                "status": "initialized",
                "components": self.intelligence_status,
                "timestamp": self.last_intelligence_update.isoformat(),
            }

        except Exception as e:
            print(f"âŒ Intelligence Layer initialization failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def initialize_system_workflows(self) -> Dict[str, Any]:
        """Initialize all core system workflow plugins"""
        try:
            print("ðŸ§© Initializing System Workflows...")

            workflows = [
                ("goal_autopilot", "30 minutes", "Resume, escalate, retry goals"),
                ("agent_sync", "4 hours", "Ensure agent memory sync"),
                ("memory_cleanser", "12 hours", "Clean low-confidence memories"),
                ("daily_reflector", "24 hours", "Generate system-wide reflection"),
                ("plugin_watchdog", "6 hours", "Monitor plugin health"),
            ]

            for workflow_name, schedule, description in workflows:
                await self._initialize_workflow(workflow_name, schedule, description)

            print("âœ… System Workflows initialized successfully")
            return {
                "status": "initialized",
                "workflows": self.workflow_status,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            print(f"âŒ System Workflows initialization failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def initialize_system_modules(self) -> Dict[str, Any]:
        """Initialize all supporting system modules"""
        try:
            print("âš™ï¸ Initializing System Modules...")

            modules = [
                ("agents", "Find/update/create agents"),
                ("goals", "Goal operations and summaries"),
                ("plugins", "Plugin health checks, status"),
                ("logger", "Event logging + querying"),
                ("memory_ops", "Raw memory search/delete/stats"),
                ("utils", "Time helpers, formatting"),
            ]

            for module_name, description in modules:
                await self._initialize_module(module_name, description)

            print("âœ… System Modules initialized successfully")
            return {
                "status": "initialized",
                "modules": self.module_status,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            print(f"âŒ System Modules initialization failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def get_intelligence_status(self) -> Dict[str, Any]:
        """Get comprehensive intelligence system status"""
        return {
            "intelligence_layer": {
                "status": self.intelligence_status,
                "health": self._calculate_intelligence_health(),
                "last_update": self.last_intelligence_update.isoformat()
                if self.last_intelligence_update
                else None,
            },
            "system_workflows": {
                "status": self.workflow_status,
                "health": self._calculate_workflow_health(),
                "active_count": sum(
                    1 for w in self.workflow_status.values() if w["active"]
                ),
            },
            "system_modules": {
                "status": self.module_status,
                "health": self._calculate_module_health(),
                "active_count": sum(
                    1 for active in self.module_status.values() if active
                ),
            },
            "overall_health": self._calculate_overall_health(),
            "timestamp": datetime.now().isoformat(),
        }

    async def run_intelligence_workflow(
        self, workflow_name: str, params: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Run a specific intelligence workflow"""
        if workflow_name not in self.workflow_status:
            return {"status": "error", "message": f"Unknown workflow: {workflow_name}"}

        try:
            print(f"ðŸš€ Running workflow: {workflow_name}")

            # Run the workflow using Aether runtime if available
            if self.aether_runtime:
                result = await self._run_aether_workflow(workflow_name, params)
            else:
                result = await self._run_simulated_workflow(workflow_name, params)

            # Update workflow status
            self.workflow_status[workflow_name]["last_run"] = datetime.now()
            self.workflow_status[workflow_name]["health"] = (
                "healthy" if result.get("success") else "error"
            )

            return result

        except Exception as e:
            print(f"âŒ Workflow {workflow_name} failed: {e}")
            self.workflow_status[workflow_name]["health"] = "error"
            return {"status": "error", "message": str(e)}

    async def perform_system_reflection(self) -> Dict[str, Any]:
        """Perform comprehensive system reflection and analysis"""
        try:
            print("ðŸ” Performing system reflection...")

            # Collect system metrics
            intelligence_status = await self.get_intelligence_status()

            # Generate insights
            insights = await self._generate_system_insights(intelligence_status)

            # Create recommendations
            recommendations = await self._generate_recommendations(
                intelligence_status, insights
            )

            reflection_report = {
                "timestamp": datetime.now().isoformat(),
                "system_status": intelligence_status,
                "insights": insights,
                "recommendations": recommendations,
                "confidence_score": self._calculate_confidence_score(
                    intelligence_status
                ),
            }

            # Cache the reflection
            self.intelligence_cache["last_reflection"] = reflection_report

            print("âœ… System reflection completed")
            return reflection_report

        except Exception as e:
            print(f"âŒ System reflection failed: {e}")
            return {"status": "error", "message": str(e)}

    async def generate_response_async(self, user_message: str) -> str:
        """Generate a response based on user input using intelligence components."""
        try:
            # Check for memory-related queries
            if self.aether_runtime and self.aether_runtime.context.memory:
                memory_results = (
                    await self.aether_runtime.context.memory.query_memories(
                        user_message
                    )
                )
                if memory_results:
                    # Process memory results into a conversational response
                    response = self._process_memory_results(
                        memory_results, user_message
                    )
                    if response:
                        return response

            # Try plugin execution for dynamic responses
            if self.aether_runtime and self.aether_runtime.context.plugins:
                plugin_response = self.aether_runtime.context.plugins.execute_chain(
                    user_message
                )
                if plugin_response:
                    return plugin_response

            # Generate a contextual response based on the user message
            return self._generate_contextual_response(user_message)
        except Exception as e:
            return f"I encountered an error while processing your request: {str(e)}"

    def _process_memory_results(
        self, memory_results: List[Dict], user_message: str
    ) -> Optional[str]:
        """Process memory query results into a conversational response."""
        try:
            if not memory_results:
                return None

            # Analyze the memory results to generate a meaningful response
            relevant_memories = []
            for memory in memory_results[:3]:  # Limit to top 3 most relevant
                content = memory.get("content", {})

                # Extract meaningful information from different memory types
                if "project_understanding" in content:
                    project_info = content["project_understanding"]
                    relevant_memories.append(
                        {
                            "type": "project",
                            "info": f"Project: {project_info.get('project_name', 'Unknown')}, "
                            f"Type: {project_info.get('project_type', 'Unknown')}, "
                            f"Phase: {project_info.get('current_phase', 'Unknown')}",
                        }
                    )
                elif "feedback_entry" in content:
                    feedback_info = content["feedback_entry"]
                    relevant_memories.append(
                        {
                            "type": "feedback",
                            "info": f"Feedback: {feedback_info.get('feedback_type', 'Unknown')} "
                            f"(Rating: {feedback_info.get('rating', 'N/A')})",
                        }
                    )
                else:
                    # Generic memory content
                    relevant_memories.append(
                        {
                            "type": "general",
                            "info": str(content)[:100] + "..."
                            if len(str(content)) > 100
                            else str(content),
                        }
                    )

            # Generate response based on memory context
            if relevant_memories:
                response_parts = ["Based on what I remember:"]

                project_memories = [
                    m for m in relevant_memories if m["type"] == "project"
                ]
                if project_memories:
                    response_parts.append(
                        f"You're working on the {project_memories[0]['info']}."
                    )

                feedback_memories = [
                    m for m in relevant_memories if m["type"] == "feedback"
                ]
                if feedback_memories:
                    response_parts.append(
                        "I've been learning from your feedback to improve my responses."
                    )

                # Add contextual help based on user message
                if any(
                    word in user_message.lower()
                    for word in ["help", "how", "what", "explain"]
                ):
                    response_parts.append(
                        "How can I assist you with your current work?"
                    )
                else:
                    response_parts.append(
                        "Is there something specific you'd like to know or work on?"
                    )

                return " ".join(response_parts)

            return None
        except Exception as e:
            print(f"âŒ Error processing memory results: {e}")
            return None

    def _generate_contextual_response(self, user_message: str) -> str:
        """Generate a contextual response based on the user message."""
        message_lower = user_message.lower()

        # Greeting responses
        if any(word in message_lower for word in ["hello", "hi", "hey", "greetings"]):
            return "Hello! I'm Lyrixa, your AI assistant for the Aetherra project. How can I help you today?"

        # Help requests
        if any(word in message_lower for word in ["help", "assist", "support"]):
            return "I'm here to help with your Aetherra project! I can assist with development tasks, answer questions, provide insights, and help manage your workflow. What would you like to work on?"

        # Status inquiries
        if any(word in message_lower for word in ["status", "progress", "update"]):
            return "I can help you check the status of your project. Would you like me to review your current progress or help with next steps?"

        # Question patterns
        if any(
            word in message_lower for word in ["what", "how", "why", "when", "where"]
        ):
            return "That's a great question! I'd be happy to help you understand this better. Could you provide more context about what you're working on?"

        # Default conversational response
        return "I understand you're reaching out. I'm here to help with your Aetherra project - whether you need technical assistance, want to discuss ideas, or need help with development tasks. What's on your mind?"

    def generate_response(self, user_message: str) -> str:
        """Synchronous wrapper for generate_response_async with LLM support."""
        try:
            # Check if we have the LLM-powered conversation manager
            if self.conversation_manager and hasattr(
                self.conversation_manager, "generate_response"
            ):
                # Use async conversation manager
                try:
                    # Try to get the current event loop
                    try:
                        asyncio.get_running_loop()
                        # If we reach here, there's a running loop
                        # Use thread executor to run async code
                        import concurrent.futures

                        def run_async():
                            new_loop = asyncio.new_event_loop()
                            try:
                                if self.conversation_manager and hasattr(
                                    self.conversation_manager, "generate_response"
                                ):
                                    return new_loop.run_until_complete(
                                        self.conversation_manager.generate_response(
                                            user_message
                                        )
                                    )
                                else:
                                    return "Conversation manager not available"
                            finally:
                                new_loop.close()

                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(run_async)
                            return future.result(timeout=30)

                    except RuntimeError:
                        # No running loop, safe to use asyncio.run
                        if self.conversation_manager and hasattr(
                            self.conversation_manager, "generate_response"
                        ):
                            return asyncio.run(
                                self.conversation_manager.generate_response(
                                    user_message
                                )
                            )
                        else:
                            return "Conversation manager not available"

                except Exception as e:
                    print(f"âš ï¸ LLM conversation manager failed: {e}")
                    # Fall back to the old system
                    pass

            # Fallback to old system if LLM conversation manager not available
            # Use a more robust approach to handle async in sync context
            try:
                # Try to get the current event loop
                asyncio.get_running_loop()
                # If we reach here, there's a running loop
                # Use thread executor to run async code
                import concurrent.futures

                def run_async():
                    new_loop = asyncio.new_event_loop()
                    try:
                        return new_loop.run_until_complete(
                            self.generate_response_async(user_message)
                        )
                    finally:
                        new_loop.close()

                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_async)
                    return future.result(timeout=10)

            except RuntimeError:
                # No running loop, safe to use asyncio.run
                return asyncio.run(self.generate_response_async(user_message))

        except Exception as e:
            print(f"âš ï¸ Error in generate_response: {e}")
            # Fallback to contextual response if async fails
            try:
                return self._generate_contextual_response(user_message)
            except Exception:
                return "I'm here to help with your Aetherra project! How can I assist you today?"

    # Private helper methods
    async def _check_semantic_memory(self):
        """Check semantic memory with confidence scoring"""
        try:
            # Check if memory_ops.aether exists and is functional
            memory_ops_path = (
                Path(self.workspace_path) / "Aetherra" / "system" / "memory_ops.aether"
            )
            if memory_ops_path.exists():
                self.intelligence_status["semantic_memory"] = True
                print("âœ… Semantic memory with confidence scoring - Active")
            else:
                print("âš ï¸ memory_ops.aether not found - Semantic memory disabled")
        except Exception as e:
            print(f"âŒ Semantic memory check failed: {e}")

    async def _initialize_system_awareness(self):
        """Initialize real-time system awareness"""
        try:
            # Check for system plugin files
            system_plugins = [
                "goal_autopilot.aether",
                "agents.aether",
                "plugins.aether",
            ]
            system_dir = Path(self.workspace_path) / "Aetherra" / "system"

            if system_dir.exists():
                available_plugins = [
                    p for p in system_plugins if (system_dir / p).exists()
                ]
                if available_plugins:
                    self.intelligence_status["system_awareness"] = True
                    print(
                        f"âœ… System awareness - Active ({len(available_plugins)} plugins)"
                    )
                else:
                    print("âš ï¸ No system plugins found - System awareness limited")
            else:
                print("âš ï¸ System directory not found - System awareness disabled")
        except Exception as e:
            print(f"âŒ System awareness initialization failed: {e}")

    async def _setup_self_reflection(self):
        """Setup AI self-reflection capabilities"""
        try:
            reflector_path = (
                Path(self.workspace_path)
                / "Aetherra"
                / "system"
                / "daily_reflector.aether"
            )
            if reflector_path.exists():
                self.intelligence_status["self_reflection"] = True
                print("âœ… AI self-reflection - Active")
            else:
                print("âš ï¸ daily_reflector.aether not found - Self-reflection disabled")
        except Exception as e:
            print(f"âŒ Self-reflection setup failed: {e}")

    async def _initialize_event_correlation(self):
        """Initialize event correlation and reasoning"""
        try:
            logger_path = (
                Path(self.workspace_path) / "Aetherra" / "system" / "logger.aether"
            )
            if logger_path.exists():
                self.intelligence_status["event_correlation"] = True
                print("âœ… Event correlation + reasoning - Active")
            else:
                print("âš ï¸ logger.aether not found - Event correlation disabled")
        except Exception as e:
            print(f"âŒ Event correlation initialization failed: {e}")

    async def _setup_conversational_integration(self):
        """Setup conversational interface integration"""
        try:
            # This is handled by Lyrixa's conversational engine
            self.intelligence_status["conversational_integration"] = True
            print("âœ… Conversational interface integration - Active")
        except Exception as e:
            print(f"âŒ Conversational integration setup failed: {e}")

    async def _initialize_plugin_monitoring(self):
        """Initialize plugin monitoring capabilities"""
        try:
            watchdog_path = (
                Path(self.workspace_path)
                / "Aetherra"
                / "system"
                / "plugin_watchdog.aether"
            )
            if watchdog_path.exists():
                self.intelligence_status["plugin_monitoring"] = True
                print("âœ… Plugin monitoring - Active")
            else:
                print("âš ï¸ plugin_watchdog.aether not found - Plugin monitoring disabled")
        except Exception as e:
            print(f"âŒ Plugin monitoring initialization failed: {e}")

    async def _initialize_workflow(
        self, workflow_name: str, schedule: str, description: str
    ):
        """Initialize a specific workflow"""
        try:
            workflow_path = (
                Path(self.workspace_path)
                / "Aetherra"
                / "system"
                / f"{workflow_name}.aether"
            )
            if workflow_path.exists():
                self.workflow_status[workflow_name]["active"] = True
                self.workflow_status[workflow_name]["health"] = "healthy"
                print(f"âœ… {workflow_name} - Active (Schedule: {schedule})")
            else:
                print(f"âš ï¸ {workflow_name}.aether not found - Workflow disabled")
        except Exception as e:
            print(f"âŒ {workflow_name} initialization failed: {e}")

    async def _initialize_module(self, module_name: str, description: str):
        """Initialize a specific system module"""
        try:
            module_path = (
                Path(self.workspace_path)
                / "Aetherra"
                / "system"
                / f"{module_name}.aether"
            )
            if module_path.exists():
                self.module_status[module_name] = True
                print(f"âœ… {module_name} - Active")
            else:
                print(f"âš ï¸ {module_name}.aether not found - Module disabled")
        except Exception as e:
            print(f"âŒ {module_name} initialization failed: {e}")

    async def _run_aether_workflow(
        self, workflow_name: str, params: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Run workflow using Aether runtime"""
        try:
            # Load and execute the workflow
            workflow_path = (
                Path(self.workspace_path)
                / "Aetherra"
                / "system"
                / f"{workflow_name}.aether"
            )
            if workflow_path.exists():
                with open(workflow_path, "r") as f:
                    workflow_code = f.read()

                # Execute using Aether runtime
                result = None
                if self.aether_runtime:
                    try:
                        # Try different execution methods
                        if hasattr(self.aether_runtime, "execute_async"):
                            # Check if it's actually async
                            execute_method = getattr(
                                self.aether_runtime, "execute_async"
                            )
                            if asyncio.iscoroutinefunction(execute_method):
                                result = await execute_method(
                                    workflow_code, params or {}
                                )
                            else:
                                result = execute_method(workflow_code, params or {})
                        elif hasattr(self.aether_runtime, "execute"):
                            # Try the sync execute method
                            execute_method = getattr(self.aether_runtime, "execute")
                            try:
                                result = execute_method(workflow_code)
                            except TypeError:
                                # If it doesn't accept parameters, try without
                                result = execute_method()
                        else:
                            result = "No execute method found on AetherRuntime"
                    except Exception as e:
                        result = f"Execution failed: {str(e)}"
                else:
                    result = "AetherRuntime not available"

                # If we still don't have a result, use fallback
                if result is None:
                    result = f"Executed {workflow_name} (simulated)"

                return {
                    "success": True,
                    "result": result,
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                return {"success": False, "error": "Workflow file not found"}

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def _run_simulated_workflow(
        self, workflow_name: str, params: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Run simulated workflow when Aether runtime is not available"""
        await asyncio.sleep(0.1)  # Simulate processing time
        return {
            "success": True,
            "result": f"Simulated execution of {workflow_name}",
            "timestamp": datetime.now().isoformat(),
            "note": "Aether runtime not available - simulated execution",
        }

    async def _generate_system_insights(self, status: Dict[str, Any]) -> List[str]:
        """Generate insights from system status"""
        insights = []

        # Check intelligence layer health
        if status["intelligence_layer"]["health"] < 0.5:
            insights.append(
                "Intelligence layer needs attention - some components are offline"
            )

        # Check workflow health
        if status["system_workflows"]["health"] < 0.5:
            insights.append("System workflows are experiencing issues")

        # Check module health
        if status["system_modules"]["health"] < 0.5:
            insights.append("System modules need maintenance")

        # Overall system health
        if status["overall_health"] > 0.8:
            insights.append("System is operating optimally")
        elif status["overall_health"] > 0.6:
            insights.append("System is operating well with minor issues")
        else:
            insights.append("System requires immediate attention")

        return insights

    async def _generate_recommendations(
        self, status: Dict[str, Any], insights: List[str]
    ) -> List[str]:
        """Generate recommendations based on status and insights"""
        recommendations = []

        # Check for offline components
        offline_components = [
            name
            for name, active in status["intelligence_layer"]["status"].items()
            if not active
        ]

        if offline_components:
            recommendations.append(
                f"Restart or repair offline components: {', '.join(offline_components)}"
            )

        # Check for workflow issues
        failed_workflows = [
            name
            for name, info in status["system_workflows"]["status"].items()
            if info["health"] == "error"
        ]

        if failed_workflows:
            recommendations.append(
                f"Investigate workflow failures: {', '.join(failed_workflows)}"
            )

        # General recommendations
        if status["overall_health"] < 0.7:
            recommendations.append(
                "Consider running system diagnostics and maintenance"
            )

        return recommendations

    def _calculate_intelligence_health(self) -> float:
        """Calculate intelligence layer health score"""
        if not self.intelligence_status:
            return 0.0

        active_count = sum(1 for active in self.intelligence_status.values() if active)
        total_count = len(self.intelligence_status)
        return active_count / total_count if total_count > 0 else 0.0

    def _calculate_workflow_health(self) -> float:
        """Calculate workflow health score"""
        if not self.workflow_status:
            return 0.0

        healthy_count = sum(
            1 for info in self.workflow_status.values() if info["health"] == "healthy"
        )
        total_count = len(self.workflow_status)
        return healthy_count / total_count if total_count > 0 else 0.0

    def _calculate_module_health(self) -> float:
        """Calculate module health score"""
        if not self.module_status:
            return 0.0

        active_count = sum(1 for active in self.module_status.values() if active)
        total_count = len(self.module_status)
        return active_count / total_count if total_count > 0 else 0.0

    def _calculate_overall_health(self) -> float:
        """Calculate overall system health score"""
        intelligence_health = self._calculate_intelligence_health()
        workflow_health = self._calculate_workflow_health()
        module_health = self._calculate_module_health()

        # Weighted average (intelligence layer is most important)
        return intelligence_health * 0.5 + workflow_health * 0.3 + module_health * 0.2

    def _calculate_confidence_score(self, status: Dict[str, Any]) -> float:
        """Calculate confidence score for system reflection"""
        overall_health = status["overall_health"]
        active_workflows = status["system_workflows"]["active_count"]
        active_modules = status["system_modules"]["active_count"]

        # Base confidence on health and active components
        confidence = overall_health * 0.6
        confidence += min(active_workflows / 5, 1.0) * 0.2  # Max 5 workflows
        confidence += min(active_modules / 6, 1.0) * 0.2  # Max 6 modules

        return min(confidence, 1.0)
